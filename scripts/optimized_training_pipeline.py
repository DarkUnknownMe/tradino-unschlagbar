#!/usr/bin/env python3
"""
ğŸ§  TRADINO OPTIMIZED TRAINING PIPELINE
Resource-efficient ML Training fÃ¼r 4GB RAM Setup
"""

import os
import sys
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

try:
    import xgboost as xgb
    import lightgbm as lgb
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import TimeSeriesSplit, cross_val_score
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    from sklearn.impute import SimpleImputer
    import pandas_ta as ta
    import yfinance as yf
    ML_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ ML libraries not available: {e}")
    ML_AVAILABLE = False

import pickle
import json
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any

class OptimizedTrainingPipeline:
    """ğŸš€ Resource-Optimized Training Pipeline"""
    
    def __init__(self):
        self.progress = 0
        self.models = {}
        self.feature_pipeline = None
        
        print("ğŸ§  Optimized Training Pipeline initialisiert")
        
        if not ML_AVAILABLE:
            print("âŒ Required ML libraries missing!")
            return
    
    def update_progress(self, step: int, total: int, description: str):
        """ğŸ“Š Update Progress Bar"""
        progress = int((step / total) * 100)
        self.progress = progress
        
        # Progress Bar
        filled = int(progress * 40 / 100)
        bar = "â–ˆ" * filled + "â–‘" * (40 - filled)
        
        print(f"\nğŸ“Š TRAINING PROGRESS")
        print(f"{bar} {progress}% Complete")
        print(f"[{description:38}] {'âœ…' if progress == 100 else 'â³'}")
    
    def collect_demo_data(self) -> pd.DataFrame:
        """ğŸ“Š Create demo training data"""
        
        self.update_progress(1, 10, "Creating demo training data...")
        
        # Create synthetic market data for demo
        np.random.seed(42)
        dates = pd.date_range(start='2023-01-01', end='2024-01-01', freq='1H')
        
        data = []
        for i, date in enumerate(dates):
            # Simulate price movement
            base_price = 50000 + 1000 * np.sin(i * 0.01) + 5000 * np.random.randn()
            
            high = base_price + abs(np.random.normal(0, 100))
            low = base_price - abs(np.random.normal(0, 100))
            close = base_price + np.random.normal(0, 50)
            volume = abs(np.random.normal(1000000, 200000))
            
            data.append({
                'Datetime': date,
                'Open': base_price,
                'High': high,
                'Low': low,
                'Close': close,
                'Volume': volume,
                'symbol': 'BTC-USD'
            })
        
        demo_df = pd.DataFrame(data)
        print(f"ğŸ“Š Demo data created: {len(demo_df)} samples")
        return demo_df
    
    def create_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ğŸ”§ Feature Engineering"""
        
        self.update_progress(2, 10, "Creating features...")
        
        df = data.copy()
        
        # Basic features
        df['returns'] = df['Close'].pct_change()
        df['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))
        
        # Technical indicators (simplified)
        df['sma_10'] = df['Close'].rolling(10).mean()
        df['sma_30'] = df['Close'].rolling(30).mean()
        df['volatility'] = df['returns'].rolling(20).std()
        
        # Simple momentum
        df['momentum_5'] = df['Close'] / df['Close'].shift(5) - 1
        df['momentum_10'] = df['Close'] / df['Close'].shift(10) - 1
        
        # Volume features
        df['volume_ma'] = df['Volume'].rolling(20).mean()
        df['volume_ratio'] = df['Volume'] / df['volume_ma']
        
        # Price position
        df['price_position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'])
        
        # Target: future return direction
        df['future_return'] = df['returns'].shift(-1)
        df['target'] = (df['future_return'] > 0).astype(int)
        
        # Clean data
        df = df.dropna()
        
        print(f"ğŸ”§ Features created: {len(df.columns)} columns")
        print(f"ğŸ“Š Clean samples: {len(df)}")
        
        return df
    
    def prepare_training_data(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """ğŸ“‹ Prepare data for training"""
        
        self.update_progress(3, 10, "Preparing training data...")
        
        feature_columns = [
            'returns', 'log_returns', 'sma_10', 'sma_30', 'volatility',
            'momentum_5', 'momentum_10', 'volume_ratio', 'price_position'
        ]
        
        # Filter available columns
        available_features = [col for col in feature_columns if col in data.columns]
        
        X = data[available_features].values
        y = data['target'].values
        
        # Handle missing values
        imputer = SimpleImputer(strategy='median')
        X = imputer.fit_transform(X)
        
        # Scale features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Store pipeline
        self.feature_pipeline = {
            'features': available_features,
            'imputer': imputer,
            'scaler': scaler
        }
        
        print(f"ğŸ“Š Training data shape: {X_scaled.shape}")
        print(f"ğŸ¯ Target distribution: {np.bincount(y)}")
        
        return X_scaled, y, available_features
    
    def train_xgboost_model(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ğŸš€ Train XGBoost Model"""
        
        self.update_progress(4, 10, "Training XGBoost...")
        
        model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=4,
            learning_rate=0.1,
            random_state=42,
            n_jobs=2
        )
        
        # Cross-validation
        tscv = TimeSeriesSplit(n_splits=3)
        cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy')
        
        # Train final model
        model.fit(X, y)
        y_pred = model.predict(X)
        
        metrics = {
            'accuracy': accuracy_score(y, y_pred),
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
        
        self.models['xgboost_trend'] = {
            'model': model,
            'type': 'xgboost',
            'metrics': metrics
        }
        
        print(f"âœ… XGBoost trained: {metrics['accuracy']:.3f} accuracy")
        return metrics
    
    def train_lightgbm_model(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """âš¡ Train LightGBM Model"""
        
        self.update_progress(5, 10, "Training LightGBM...")
        
        model = lgb.LGBMClassifier(
            n_estimators=100,
            max_depth=4,
            learning_rate=0.1,
            random_state=42,
            n_jobs=2,
            verbosity=-1
        )
        
        # Cross-validation
        tscv = TimeSeriesSplit(n_splits=3)
        cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy')
        
        # Train final model
        model.fit(X, y)
        y_pred = model.predict(X)
        
        metrics = {
            'accuracy': accuracy_score(y, y_pred),
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
        
        self.models['lightgbm_volatility'] = {
            'model': model,
            'type': 'lightgbm',
            'metrics': metrics
        }
        
        print(f"âœ… LightGBM trained: {metrics['accuracy']:.3f} accuracy")
        return metrics
    
    def train_random_forest_model(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ğŸŒ² Train Random Forest Model"""
        
        self.update_progress(6, 10, "Training Random Forest...")
        
        model = RandomForestClassifier(
            n_estimators=50,
            max_depth=8,
            random_state=42,
            n_jobs=2
        )
        
        # Cross-validation
        tscv = TimeSeriesSplit(n_splits=3)
        cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy')
        
        # Train final model
        model.fit(X, y)
        y_pred = model.predict(X)
        
        metrics = {
            'accuracy': accuracy_score(y, y_pred),
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
        
        self.models['random_forest_risk'] = {
            'model': model,
            'type': 'random_forest',
            'metrics': metrics
        }
        
        print(f"âœ… Random Forest trained: {metrics['accuracy']:.3f} accuracy")
        return metrics
    
    def create_ensemble_model(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """ğŸ­ Create Ensemble Model"""
        
        self.update_progress(7, 10, "Creating ensemble...")
        
        if len(self.models) < 2:
            print("âš ï¸ Not enough models for ensemble")
            return {}
        
        # Get predictions from all models
        predictions = []
        for model_name, model_info in self.models.items():
            pred_proba = model_info['model'].predict_proba(X)[:, 1]
            predictions.append(pred_proba)
        
        # Simple ensemble: average
        ensemble_pred_proba = np.mean(predictions, axis=0)
        ensemble_pred = (ensemble_pred_proba > 0.5).astype(int)
        
        metrics = {
            'accuracy': accuracy_score(y, ensemble_pred)
        }
        
        self.models['ensemble'] = {
            'type': 'ensemble',
            'metrics': metrics,
            'num_models': len(predictions)
        }
        
        print(f"âœ… Ensemble created: {metrics['accuracy']:.3f} accuracy")
        return metrics
    
    def save_models(self):
        """ğŸ’¾ Save models"""
        
        self.update_progress(8, 10, "Saving models...")
        
        os.makedirs('models', exist_ok=True)
        
        # Save individual models
        for name, info in self.models.items():
            if 'model' in info:
                with open(f'models/{name}.pkl', 'wb') as f:
                    pickle.dump(info['model'], f)
        
        # Save feature pipeline
        with open('models/feature_pipeline.pkl', 'wb') as f:
            pickle.dump(self.feature_pipeline, f)
        
        # Save model info
        model_info = {}
        for name, info in self.models.items():
            model_info[name] = {
                'type': info['type'],
                'metrics': info['metrics']
            }
        
        with open('models/model_info.json', 'w') as f:
            json.dump(model_info, f, indent=2)
        
        print("âœ… Models saved successfully")
    
    def run_complete_training(self):
        """ğŸš€ Run complete training pipeline"""
        
        if not ML_AVAILABLE:
            print("âŒ Cannot run training - missing dependencies")
            return
        
        print("ğŸš€ STARTING OPTIMIZED TRAINING")
        print("=" * 40)
        
        try:
            # Collect data
            data = self.collect_demo_data()
            
            # Create features
            features_data = self.create_features(data)
            
            # Prepare training data
            X, y, feature_names = self.prepare_training_data(features_data)
            
            # Train models
            self.train_xgboost_model(X, y)
            self.train_lightgbm_model(X, y)
            self.train_random_forest_model(X, y)
            
            # Create ensemble
            self.create_ensemble_model(X, y)
            
            # Save models
            self.save_models()
            
            # Complete
            self.update_progress(10, 10, "Training completed!")
            
            self.print_summary()
            
        except Exception as e:
            print(f"âŒ Training failed: {e}")
            import traceback
            traceback.print_exc()
    
    def print_summary(self):
        """ğŸ“Š Print summary"""
        
        print("\n" + "="*50)
        print("ğŸ“Š TRAINING SUMMARY")
        print("="*50)
        
        for name, info in self.models.items():
            if 'metrics' in info:
                acc = info['metrics']['accuracy']
                print(f"ğŸ¤– {name}: {acc:.3f} accuracy")
        
        print(f"\nâœ… Training complete!")
        print(f"ğŸ“ Models saved: {len(self.models)}")
        print(f"ğŸ”§ Features: {len(self.feature_pipeline['features'])}")
        print("ğŸš€ Ready for trading!")

if __name__ == "__main__":
    pipeline = OptimizedTrainingPipeline()
    pipeline.run_complete_training() 