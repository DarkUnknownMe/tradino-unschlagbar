import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from typing import Dict, List, Tuple, Optional, Any, Union
from loguru import logger
from datetime import datetime
from collections import deque, namedtuple
import asyncio
import random
from dataclasses import dataclass
from enum import Enum
import math

# Advanced RL Libraries
try:
    from stable_baselines3 import PPO, SAC, TD3, A2C
    from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
    from stable_baselines3.common.callbacks import BaseCallback
    from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise
    from stable_baselines3.common.buffers import ReplayBuffer
    SB3_AVAILABLE = True
    logger.success("‚úÖ Stable Baselines3 Advanced RL verf√ºgbar")
except ImportError:
    logger.warning("‚ö†Ô∏è Stable Baselines3 nicht verf√ºgbar - verwende Custom Implementation")
    SB3_AVAILABLE = False

class RLAlgorithm(Enum):
    """
    Verf√ºgbare RL Algorithmen
    """
    PPO = "ppo"                    # Proximal Policy Optimization
    SAC = "sac"                    # Soft Actor-Critic
    TD3 = "td3"                    # Twin Delayed Deep Deterministic Policy Gradient
    A3C = "a3c"                    # Asynchronous Actor-Critic
    DDPG = "ddpg"                  # Deep Deterministic Policy Gradient
    RAINBOW_DQN = "rainbow_dqn"    # Rainbow Deep Q-Network
    IMPALA = "impala"              # Importance Weighted Actor-Learner Architecture
    APEX_DQN = "apex_dqn"          # Distributed Prioritized Experience Replay

@dataclass
class RLPerformanceMetrics:
    """
    RL Performance Metrics
    """
    algorithm: str
    total_episodes: int
    average_reward: float
    best_episode_reward: float
    win_rate: float
    sharpe_ratio: float
    max_drawdown: float
    training_time: float
    inference_time: float
    convergence_episode: int
    stability_score: float

class Experience:
    """
    Experience Tuple f√ºr Replay Buffer
    """
    def __init__(self, state, action, reward, next_state, done, info=None):
        self.state = state
        self.action = action
        self.reward = reward
        self.next_state = next_state
        self.done = done
        self.info = info or {}

class TRADINOAdvancedRLSuite:
    """
    Advanced Reinforcement Learning Suite f√ºr TRADINO
    Multi-Algorithm Ensemble mit PPO, SAC, TD3, A3C und mehr
    """
    
    def __init__(self, config, trading_environment=None):
        self.config = config
        self.trading_env = trading_environment
        
        # RL Configuration
        self.enabled_algorithms = config.get('rl_algorithms', [
            RLAlgorithm.PPO,
            RLAlgorithm.SAC,
            RLAlgorithm.TD3,
            RLAlgorithm.A3C
        ])
        
        # Algorithm Registry
        self.algorithms = {}
        self.algorithm_performance = {}
        self.algorithm_weights = {}
        
        # Ensemble Configuration
        self.ensemble_mode = config.get('rl_ensemble_mode', 'weighted_voting')
        self.meta_learner = None
        
        # Training Configuration
        self.total_timesteps = config.get('rl_total_timesteps', 100000)
        self.parallel_envs = config.get('rl_parallel_envs', 4)
        self.update_frequency = config.get('rl_update_frequency', 1000)
        
        # Experience Replay
        self.shared_replay_buffer = deque(maxlen=100000)
        self.prioritized_replay = config.get('rl_prioritized_replay', True)
        
        # Multi-Agent RL
        self.multi_agent_mode = config.get('rl_multi_agent', False)
        self.agent_communication = config.get('rl_agent_communication', True)
        
        # Performance Tracking
        self.performance_history = deque(maxlen=10000)
        self.convergence_tracker = {}
        
        # Device Selection
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        logger.info(f"üß† Advanced RL Suite initialisiert mit {len(self.enabled_algorithms)} Algorithmen")
    
    async def initialize(self):
        """
        Advanced RL Suite initialisieren
        """
        try:
            logger.info("üöÄ Advanced RL Suite Initialisierung startet...")
            
            # Initialize Algorithms
            for algorithm in self.enabled_algorithms:
                success = await self._initialize_algorithm(algorithm)
                if success:
                    logger.success(f"‚úÖ {algorithm.value.upper()} Algorithm initialisiert")
                else:
                    logger.warning(f"‚ö†Ô∏è {algorithm.value.upper()} Algorithm Initialisierung fehlgeschlagen")
            
            # Initialize Meta-Learner
            if len(self.algorithms) > 1:
                await self._initialize_meta_learner()
            
            # Initialize Ensemble Weights
            await self._initialize_ensemble_weights()
            
            logger.success(f"‚úÖ Advanced RL Suite bereit mit {len(self.algorithms)} aktiven Algorithmen")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Advanced RL Suite Initialization Fehler: {e}")
            return False
    
    async def _initialize_algorithm(self, algorithm: RLAlgorithm) -> bool:
        """
        Einzelnen RL Algorithmus initialisieren
        """
        try:
            if SB3_AVAILABLE:
                agent = await self._create_sb3_agent(algorithm)
            else:
                agent = await self._create_custom_agent(algorithm)
            
            if agent:
                self.algorithms[algorithm] = agent
                self.algorithm_performance[algorithm] = RLPerformanceMetrics(
                    algorithm=algorithm.value,
                    total_episodes=0,
                    average_reward=0.0,
                    best_episode_reward=-np.inf,
                    win_rate=0.0,
                    sharpe_ratio=0.0,
                    max_drawdown=0.0,
                    training_time=0.0,
                    inference_time=0.0,
                    convergence_episode=0,
                    stability_score=0.0
                )
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"‚ùå {algorithm.value} Initialization Fehler: {e}")
            return False
    
    async def _create_sb3_agent(self, algorithm: RLAlgorithm):
        """
        Stable Baselines3 Agent erstellen
        """
        try:
            # Mock environment for initialization
            if not self.trading_env:
                self.trading_env = MockTradingEnvironment()
            
            # Vectorized Environment
            vec_env = DummyVecEnv([lambda: self.trading_env])
            
            # Algorithm-specific configurations
            if algorithm == RLAlgorithm.PPO:
                agent = PPO(
                    'MlpPolicy',
                    vec_env,
                    learning_rate=3e-4,
                    n_steps=2048,
                    batch_size=64,
                    gamma=0.99,
                    gae_lambda=0.95,
                    clip_range=0.2,
                    ent_coef=0.01,
                    vf_coef=0.5,
                    max_grad_norm=0.5,
                    policy_kwargs=dict(
                        net_arch=[256, 256, 128],
                        activation_fn=nn.ReLU
                    ),
                    verbose=0,
                    device=self.device
                )
                
            elif algorithm == RLAlgorithm.SAC:
                agent = SAC(
                    'MlpPolicy',
                    vec_env,
                    learning_rate=3e-4,
                    buffer_size=1000000,
                    learning_starts=100,
                    batch_size=256,
                    tau=0.005,
                    gamma=0.99,
                    train_freq=1,
                    gradient_steps=1,
                    ent_coef='auto',
                    policy_kwargs=dict(
                        net_arch=[256, 256, 128],
                        activation_fn=nn.ReLU
                    ),
                    verbose=0,
                    device=self.device
                )
                
            elif algorithm == RLAlgorithm.TD3:
                # Action noise for TD3
                action_noise = NormalActionNoise(
                    mean=np.zeros(vec_env.action_space.shape[0]),
                    sigma=0.1 * np.ones(vec_env.action_space.shape[0])
                )
                
                agent = TD3(
                    'MlpPolicy',
                    vec_env,
                    action_noise=action_noise,
                    learning_rate=3e-4,
                    buffer_size=1000000,
                    learning_starts=100,
                    batch_size=100,
                    tau=0.005,
                    gamma=0.99,
                    train_freq=(1, "episode"),
                    gradient_steps=-1,
                    policy_delay=2,
                    target_policy_noise=0.2,
                    target_noise_clip=0.5,
                    policy_kwargs=dict(
                        net_arch=[256, 256, 128],
                        activation_fn=nn.ReLU
                    ),
                    verbose=0,
                    device=self.device
                )
                
            elif algorithm == RLAlgorithm.A3C:
                # A2C as proxy for A3C (similar algorithm)
                agent = A2C(
                    'MlpPolicy',
                    vec_env,
                    learning_rate=7e-4,
                    n_steps=5,
                    gamma=0.99,
                    gae_lambda=1.0,
                    ent_coef=0.01,
                    vf_coef=0.25,
                    max_grad_norm=0.5,
                    policy_kwargs=dict(
                        net_arch=[256, 256, 128],
                        activation_fn=nn.ReLU
                    ),
                    verbose=0,
                    device=self.device
                )
                
            else:
                logger.warning(f"‚ö†Ô∏è {algorithm.value} nicht unterst√ºtzt in SB3 Mode")
                return None
            
            return agent
            
        except Exception as e:
            logger.error(f"‚ùå SB3 {algorithm.value} Creation Fehler: {e}")
            return None
    
    async def _create_custom_agent(self, algorithm: RLAlgorithm):
        """
        Custom RL Agent erstellen (Fallback)
        """
        try:
            if algorithm == RLAlgorithm.PPO:
                return CustomPPOAgent(self.config, self.device)
            elif algorithm == RLAlgorithm.SAC:
                return CustomSACAgent(self.config, self.device)
            elif algorithm == RLAlgorithm.TD3:
                return CustomTD3Agent(self.config, self.device)
            elif algorithm == RLAlgorithm.DDPG:
                return CustomDDPGAgent(self.config, self.device)
            else:
                return CustomPPOAgent(self.config, self.device)  # Fallback
                
        except Exception as e:
            logger.error(f"‚ùå Custom {algorithm.value} Creation Fehler: {e}")
            return None
    
    async def _initialize_meta_learner(self):
        """
        Meta-Learner f√ºr Algorithm Selection initialisieren
        """
        try:
            self.meta_learner = MetaLearnerAgent(
                num_algorithms=len(self.algorithms),
                state_dim=50,  # Market state dimension
                device=self.device
            )
            
            logger.success("‚úÖ Meta-Learner f√ºr Algorithm Selection initialisiert")
            
        except Exception as e:
            logger.error(f"‚ùå Meta-Learner Initialization Fehler: {e}")
    
    async def _initialize_ensemble_weights(self):
        """
        Ensemble Weights initialisieren
        """
        try:
            # Equal weights initially
            num_algorithms = len(self.algorithms)
            if num_algorithms > 0:
                equal_weight = 1.0 / num_algorithms
                for algorithm in self.algorithms.keys():
                    self.algorithm_weights[algorithm] = equal_weight
            
            logger.info(f"üéØ Ensemble Weights initialisiert: {num_algorithms} Algorithmen mit je {equal_weight:.3f}")
            
        except Exception as e:
            logger.error(f"‚ùå Ensemble Weights Initialization Fehler: {e}")
    
    async def get_ensemble_action(self, market_state):
        """
        Ensemble Action von allen Algorithmen
        """
        try:
            actions = {}
            action_weights = {}
            
            # Get Actions from all Algorithms
            for algorithm, agent in self.algorithms.items():
                try:
                    if SB3_AVAILABLE and hasattr(agent, 'predict'):
                        action, _ = agent.predict(market_state, deterministic=False)
                    else:
                        action = await agent.get_action(market_state)
                    
                    actions[algorithm] = action
                    action_weights[algorithm] = self.algorithm_weights.get(algorithm, 0.0)
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è {algorithm.value} Action Generation Fehler: {e}")
            
            if not actions:
                return np.array([0.0])  # Neutral action
            
            # Ensemble Methods
            if self.ensemble_mode == 'weighted_voting':
                ensemble_action = await self._weighted_voting_ensemble(actions, action_weights)
            elif self.ensemble_mode == 'meta_learning':
                ensemble_action = await self._meta_learning_ensemble(actions, market_state)
            elif self.ensemble_mode == 'majority_voting':
                ensemble_action = await self._majority_voting_ensemble(actions)
            else:
                ensemble_action = await self._weighted_voting_ensemble(actions, action_weights)
            
            return ensemble_action
            
        except Exception as e:
            logger.error(f"‚ùå Ensemble Action Generation Fehler: {e}")
            return np.array([0.0])
    
    async def _weighted_voting_ensemble(self, actions: Dict, weights: Dict):
        """
        Weighted Voting Ensemble
        """
        try:
            weighted_action = 0.0
            total_weight = 0.0
            
            for algorithm, action in actions.items():
                weight = weights.get(algorithm, 0.0)
                action_value = action[0] if isinstance(action, (list, np.ndarray)) else action
                
                weighted_action += action_value * weight
                total_weight += weight
            
            if total_weight > 0:
                final_action = weighted_action / total_weight
            else:
                final_action = 0.0
            
            return np.array([final_action])
            
        except Exception as e:
            logger.error(f"‚ùå Weighted Voting Fehler: {e}")
            return np.array([0.0])
    
    async def _majority_voting_ensemble(self, actions: Dict):
        """
        Majority Voting Ensemble
        """
        try:
            # Convert continuous actions to discrete votes
            votes = {'BUY': 0, 'SELL': 0, 'HOLD': 0}
            
            for action in actions.values():
                action_value = action[0] if isinstance(action, (list, np.ndarray)) else action
                
                if action_value > 0.1:
                    votes['BUY'] += 1
                elif action_value < -0.1:
                    votes['SELL'] += 1
                else:
                    votes['HOLD'] += 1
            
            # Majority Decision
            majority_action = max(votes, key=votes.get)
            
            action_mapping = {'BUY': 0.5, 'SELL': -0.5, 'HOLD': 0.0}
            return np.array([action_mapping[majority_action]])
            
        except Exception as e:
            logger.error(f"‚ùå Majority Voting Fehler: {e}")
            return np.array([0.0])
    
    async def _meta_learning_ensemble(self, actions: Dict, market_state):
        """
        Meta-Learning basierte Ensemble Auswahl
        """
        try:
            if not self.meta_learner:
                return await self._weighted_voting_ensemble(actions, self.algorithm_weights)
            
            # Meta-Learner w√§hlt besten Algorithmus
            best_algorithm = await self.meta_learner.select_algorithm(market_state)
            
            if best_algorithm in actions:
                selected_action = actions[best_algorithm]
                return selected_action if isinstance(selected_action, np.ndarray) else np.array([selected_action])
            else:
                return await self._weighted_voting_ensemble(actions, self.algorithm_weights)
                
        except Exception as e:
            logger.error(f"‚ùå Meta-Learning Ensemble Fehler: {e}")
            return await self._weighted_voting_ensemble(actions, self.algorithm_weights)
    
    async def get_trading_signal(self, market_observation):
        """
        Trading Signal f√ºr Integration mit TRADINO
        """
        try:
            # Get Ensemble Action
            ensemble_action = await self.get_ensemble_action(market_observation)
            
            # Convert to Trading Signal
            action_value = ensemble_action[0] if len(ensemble_action) > 0 else 0.0
            
            # Signal Classification
            if action_value > 0.2:
                action = 'BUY'
                confidence = min(1.0, abs(action_value) * 2)
            elif action_value < -0.2:
                action = 'SELL'
                confidence = min(1.0, abs(action_value) * 2)
            else:
                action = 'HOLD'
                confidence = 0.3
            
            # Position Size
            position_size = min(0.2, abs(action_value) * 0.4)  # Max 20%, scaled by action strength
            
            # Algorithm Consensus
            algorithm_consensus = await self._calculate_algorithm_consensus()
            
            trading_signal = {
                'action': action,
                'confidence': confidence,
                'position_size': position_size,
                'source': 'Advanced_RL_Ensemble',
                'ensemble_mode': self.ensemble_mode,
                'active_algorithms': len(self.algorithms),
                'algorithm_consensus': algorithm_consensus,
                'raw_action_value': action_value,
                'algorithm_weights': self.algorithm_weights.copy(),
                'timestamp': datetime.now()
            }
            
            return trading_signal
            
        except Exception as e:
            logger.error(f"‚ùå Trading Signal Generation Fehler: {e}")
            return {
                'action': 'HOLD',
                'confidence': 0.0,
                'position_size': 0.0,
                'source': 'Advanced_RL_Error',
                'error': str(e)
            }
    
    async def _calculate_algorithm_consensus(self):
        """
        Algorithm Consensus Score berechnen
        """
        try:
            if len(self.algorithms) < 2:
                return 1.0
            
            # Get recent actions from all algorithms
            mock_state = np.random.randn(50)  # Mock state for consensus calculation
            actions = []
            
            for algorithm, agent in self.algorithms.items():
                try:
                    if SB3_AVAILABLE and hasattr(agent, 'predict'):
                        action, _ = agent.predict(mock_state, deterministic=True)
                    else:
                        action = await agent.get_action(mock_state)
                    
                    action_value = action[0] if isinstance(action, (list, np.ndarray)) else action
                    actions.append(action_value)
                    
                except:
                    continue
            
            if len(actions) < 2:
                return 0.5
            
            # Calculate consensus as inverse of standard deviation
            action_std = np.std(actions)
            consensus_score = 1.0 / (1.0 + action_std)
            
            return min(1.0, max(0.0, consensus_score))
            
        except Exception as e:
            logger.error(f"‚ùå Algorithm Consensus Calculation Fehler: {e}")
            return 0.5
    
    def get_performance_summary(self):
        """
        Performance Summary aller Algorithmen
        """
        try:
            summary = {
                'active_algorithms': len(self.algorithms),
                'ensemble_mode': self.ensemble_mode,
                'total_training_time': 0.0,
                'algorithms': {}
            }
            
            for algorithm, performance in self.algorithm_performance.items():
                summary['algorithms'][algorithm.value] = {
                    'episodes': performance.total_episodes,
                    'average_reward': performance.average_reward,
                    'best_reward': performance.best_episode_reward,
                    'win_rate': performance.win_rate,
                    'sharpe_ratio': performance.sharpe_ratio,
                    'stability_score': performance.stability_score,
                    'training_time': performance.training_time,
                    'weight': self.algorithm_weights.get(algorithm, 0.0)
                }
                
                summary['total_training_time'] += performance.training_time
            
            # Best Performing Algorithm
            if self.algorithm_performance:
                best_algorithm = max(
                    self.algorithm_performance.keys(),
                    key=lambda alg: self.algorithm_performance[alg].average_reward
                )
                summary['best_algorithm'] = best_algorithm.value
                summary['best_performance'] = self.algorithm_performance[best_algorithm].average_reward
            
            return summary
            
        except Exception as e:
            logger.error(f"‚ùå Performance Summary Fehler: {e}")
            return {'error': str(e)}


# Custom RL Agents (Fallback Implementations)

class CustomPPOAgent:
    """
    Custom PPO Implementation
    """
    
    def __init__(self, config, device):
        self.config = config
        self.device = device
        self.learning_rate = 3e-4
        
        # Networks
        self.actor = self._build_actor_network().to(device)
        self.critic = self._build_critic_network().to(device)
        
        # Optimizers
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.learning_rate)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.learning_rate)
        
        # PPO Parameters
        self.clip_ratio = 0.2
        self.entropy_coef = 0.01
        self.value_coef = 0.5
        
    def _build_actor_network(self):
        return nn.Sequential(
            nn.Linear(50, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Tanh()
        )
    
    def _build_critic_network(self):
        return nn.Sequential(
            nn.Linear(50, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )
    
    async def get_action(self, state, deterministic=False):
        try:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                action_mean = self.actor(state_tensor)
                
                if deterministic:
                    action = action_mean
                else:
                    noise = torch.normal(0, 0.1, action_mean.shape).to(self.device)
                    action = action_mean + noise
                    action = torch.clamp(action, -1, 1)
            
            return action.cpu().numpy().flatten()
            
        except Exception as e:
            logger.error(f"‚ùå Custom PPO Action Fehler: {e}")
            return np.array([0.0])


class CustomSACAgent:
    """
    Custom SAC Implementation
    """
    
    def __init__(self, config, device):
        self.config = config
        self.device = device
        self.learning_rate = 3e-4
        
        # Networks
        self.actor = self._build_actor_network().to(device)
        
        # SAC Parameters
        self.tau = 0.005
        self.gamma = 0.99
        self.alpha = 0.2  # Entropy coefficient
    
    def _build_actor_network(self):
        return nn.Sequential(
            nn.Linear(50, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 2),  # Mean and log_std
        )
    
    async def get_action(self, state, deterministic=False):
        try:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                actor_output = self.actor(state_tensor)
                mean = actor_output[:, 0]
                log_std = actor_output[:, 1]
                
                if deterministic:
                    action = torch.tanh(mean)
                else:
                    std = torch.exp(log_std)
                    normal = torch.distributions.Normal(mean, std)
                    action = torch.tanh(normal.sample())
            
            return action.cpu().numpy().flatten()
            
        except Exception as e:
            logger.error(f"‚ùå Custom SAC Action Fehler: {e}")
            return np.array([0.0])


class CustomTD3Agent:
    """
    Custom TD3 Implementation
    """
    
    def __init__(self, config, device):
        self.config = config
        self.device = device
        self.learning_rate = 3e-4
        
        # Networks
        self.actor = self._build_actor_network().to(device)
        
        # TD3 Parameters
        self.tau = 0.005
        self.gamma = 0.99
        self.policy_noise = 0.2
        self.noise_clip = 0.5
        self.policy_delay = 2
        self.update_counter = 0
    
    def _build_actor_network(self):
        return nn.Sequential(
            nn.Linear(50, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Tanh()
        )
    
    async def get_action(self, state, deterministic=False):
        try:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                action = self.actor(state_tensor)
                
                if not deterministic:
                    noise = torch.normal(0, 0.1, action.shape).to(self.device)
                    action = action + noise
                    action = torch.clamp(action, -1, 1)
            
            return action.cpu().numpy().flatten()
            
        except Exception as e:
            logger.error(f"‚ùå Custom TD3 Action Fehler: {e}")
            return np.array([0.0])


class CustomDDPGAgent:
    """
    Custom DDPG Implementation
    """
    
    def __init__(self, config, device):
        self.config = config
        self.device = device
        self.learning_rate = 1e-4
        
        # Networks
        self.actor = self._build_actor_network().to(device)
        
        # DDPG Parameters
        self.tau = 0.001
        self.gamma = 0.99
    
    def _build_actor_network(self):
        return nn.Sequential(
            nn.Linear(50, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Tanh()
        )
    
    async def get_action(self, state, deterministic=False):
        try:
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                action = self.actor(state_tensor)
                
                if not deterministic:
                    noise = torch.normal(0, 0.1, action.shape).to(self.device)
                    action = action + noise
                    action = torch.clamp(action, -1, 1)
            
            return action.cpu().numpy().flatten()
            
        except Exception as e:
            logger.error(f"‚ùå Custom DDPG Action Fehler: {e}")
            return np.array([0.0])


class MetaLearnerAgent:
    """
    Meta-Learner f√ºr Algorithm Selection
    """
    
    def __init__(self, num_algorithms: int, state_dim: int, device):
        self.num_algorithms = num_algorithms
        self.state_dim = state_dim
        self.device = device
        
        # Meta-Network
        self.meta_network = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, num_algorithms),
            nn.Softmax(dim=1)
        ).to(device)
        
        self.optimizer = optim.Adam(self.meta_network.parameters(), lr=1e-4)
        
    async def select_algorithm(self, market_state):
        """
        Best Algorithm f√ºr Market State ausw√§hlen
        """
        try:
            state_tensor = torch.FloatTensor(market_state).to(self.device)
            
            with torch.no_grad():
                probabilities = self.meta_network(state_tensor.unsqueeze(0))
                best_algorithm_idx = torch.argmax(probabilities).item()
                
                algorithms = list(RLAlgorithm)
                return algorithms[best_algorithm_idx]
                
        except Exception as e:
            logger.error(f"‚ùå Meta-Learner Algorithm Selection Fehler: {e}")
            return RLAlgorithm.PPO  # Fallback


class MockTradingEnvironment:
    """
    Mock Trading Environment f√ºr Testing
    """
    
    def __init__(self):
        self.observation_space = type('MockSpace', (), {'shape': (50,)})()
        self.action_space = type('MockSpace', (), {'shape': (1,)})()
        self.current_step = 0
        
    def reset(self):
        self.current_step = 0
        return np.random.randn(50)
    
    def step(self, action):
        self.current_step += 1
        next_state = np.random.randn(50)
        reward = np.random.randn() * 0.1
        done = self.current_step >= 100
        info = {}
        return next_state, reward, done, info


# Integration f√ºr TRADINO System
class AdvancedRLIntegration:
    """
    Integration der Advanced RL Suite in das TRADINO Trading System
    """
    
    def __init__(self, config, trading_engine=None):
        self.config = config
        self.trading_engine = trading_engine
        
        # RL Environment Setup
        self.trading_env = MockTradingEnvironment()
        
        # Advanced RL Suite
        self.rl_suite = TRADINOAdvancedRLSuite(config, self.trading_env)
        
        # Integration Settings
        self.rl_enabled = config.get('advanced_rl_enabled', True)
        self.rl_signal_weight = config.get('advanced_rl_signal_weight', 0.35)  # 35% Gewichtung
        
        # Performance Tracking
        self.integration_performance = {
            'signals_generated': 0,
            'successful_predictions': 0,
            'ensemble_accuracy': 0.0,
            'best_algorithm': None,
            'last_signal': None
        }
        
        logger.info("üöÄ Advanced RL Integration initialisiert")
    
    async def initialize(self):
        """
        Advanced RL Integration initialisieren
        """
        try:
            if self.rl_enabled:
                # Initialize RL Suite
                success = await self.rl_suite.initialize()
                
                if success:
                    logger.success("‚úÖ Advanced RL Integration bereit")
                else:
                    logger.warning("‚ö†Ô∏è Advanced RL Suite Initialisierung fehlgeschlagen")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Advanced RL Integration Initialization Fehler: {e}")
            return False
    
    async def get_advanced_rl_signal(self, market_data):
        """
        Advanced RL Signal f√ºr Trading Strategy Integration
        """
        try:
            if not self.rl_enabled:
                return None
            
            # Market Observation erstellen
            market_observation = await self._create_advanced_market_observation(market_data)
            
            # Advanced RL Ensemble Signal
            rl_signal = await self.rl_suite.get_trading_signal(market_observation)
            
            # Signal Enhancement
            enhanced_signal = await self._enhance_rl_signal(rl_signal, market_data)
            
            # Performance Tracking
            self.integration_performance['signals_generated'] += 1
            self.integration_performance['last_signal'] = enhanced_signal
            
            return enhanced_signal
            
        except Exception as e:
            logger.error(f"‚ùå Advanced RL Signal Generation Fehler: {e}")
            return None
    
    async def _create_advanced_market_observation(self, market_data):
        """
        Advanced Market Observation f√ºr RL Suite erstellen
        """
        try:
            # Create 50-dimensional observation
            observation = []
            
            # Basic Market Features
            observation.extend([
                market_data.get('close', 50000) / 50000,  # Normalized price
                market_data.get('volume', 1000000) / 1000000,  # Normalized volume
                market_data.get('volatility', 0.02),  # Volatility
                (market_data.get('high', 50000) - market_data.get('low', 50000)) / market_data.get('close', 50000)  # Range
            ])
            
            # Technical Indicators (mock)
            observation.extend([0.5, 0.0, 0.5])  # RSI, MACD, BB position
            
            # Sentiment Data (mock)
            observation.extend([0.0, 0.0])  # News, Social sentiment
            
            # Market Regime (mock)
            observation.extend([0.5, 0.5])  # Regime, Confidence
            
            # Portfolio State (mock)
            observation.extend([0.0, 0.0, 0.0])  # Position, PnL, Return
            
            # Risk Metrics
            risk_score = min(1.0, market_data.get('volatility', 0.02) / 0.1)
            observation.append(risk_score)
            
            # Time Features
            import datetime
            now = datetime.datetime.now()
            observation.extend([
                now.hour / 24.0,  # Hour of day
                now.weekday() / 7.0,  # Day of week
                np.sin(2 * np.pi * now.hour / 24),  # Cyclical hour
                np.cos(2 * np.pi * now.hour / 24)
            ])
            
            # Pad or trim to exactly 50 features
            while len(observation) < 50:
                observation.append(0.0)
            observation = observation[:50]
            
            return np.array(observation, dtype=np.float32)
            
        except Exception as e:
            logger.error(f"‚ùå Advanced Market Observation Creation Fehler: {e}")
            return np.zeros(50, dtype=np.float32)
    
    async def _enhance_rl_signal(self, rl_signal, market_data):
        """
        RL Signal Enhancement
        """
        try:
            enhanced_signal = rl_signal.copy()
            
            # Signal Weight
            enhanced_signal['weight'] = self.rl_signal_weight
            
            # Priority basierend auf Algorithm Consensus
            consensus = enhanced_signal.get('algorithm_consensus', 0.5)
            if consensus > 0.8:
                enhanced_signal['priority'] = 'high'
            elif consensus > 0.6:
                enhanced_signal['priority'] = 'medium'
            else:
                enhanced_signal['priority'] = 'low'
            
            # Enhanced Metadata
            enhanced_signal['source_type'] = 'advanced_rl_ensemble'
            enhanced_signal['integration_version'] = '2.0'
            
            # Performance Context
            performance_summary = self.rl_suite.get_performance_summary()
            enhanced_signal['ensemble_performance'] = {
                'active_algorithms': performance_summary.get('active_algorithms', 0),
                'best_algorithm': performance_summary.get('best_algorithm', 'unknown'),
                'ensemble_mode': performance_summary.get('ensemble_mode', 'weighted_voting')
            }
            
            return enhanced_signal
            
        except Exception as e:
            logger.error(f"‚ùå RL Signal Enhancement Fehler: {e}")
            return rl_signal
    
    def get_rl_status(self):
        """
        Advanced RL System Status
        """
        try:
            if not self.rl_enabled:
                return {'advanced_rl_enabled': False}
            
            # Performance Summary
            performance_summary = self.rl_suite.get_performance_summary()
            
            status = {
                'advanced_rl_enabled': True,
                'ensemble_mode': performance_summary.get('ensemble_mode', 'weighted_voting'),
                'active_algorithms': performance_summary.get('active_algorithms', 0),
                'best_algorithm': performance_summary.get('best_algorithm', 'unknown'),
                'integration_performance': self.integration_performance.copy(),
                'algorithm_details': performance_summary.get('algorithms', {}),
                'signal_weight': self.rl_signal_weight
            }
            
            return status
            
        except Exception as e:
            logger.error(f"‚ùå RL Status Generation Fehler: {e}")
            return {'advanced_rl_enabled': False, 'error': str(e)} 